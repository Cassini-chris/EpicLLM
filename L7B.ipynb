{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994d6e7d-44bd-44e3-847c-712f2f1cbd0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  4 23:39:41 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.99                 Driver Version: 536.99       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 38%   33C    P8              15W / 220W |   3613MiB /  8192MiB |     20%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      6992    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7856    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      8412    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9392    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      9864    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe    N/A      |\n",
      "|    0   N/A  N/A     12484    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13628    C+G   ...ects 2023\\Support Files\\AfterFX.exe    N/A      |\n",
      "|    0   N/A  N/A     13716    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     13804    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     14744    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     16340    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     16364    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16712    C+G   ... Stream\\85.0.34.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     18700    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     18904    C+G   ...les (x86)\\Battle.net\\Battle.net.exe    N/A      |\n",
      "|    0   N/A  N/A     19044    C+G   ...es (x86)\\Dropbox\\Client\\Dropbox.exe    N/A      |\n",
      "|    0   N/A  N/A     20116    C+G   C:\\Program Files\\NordVPN\\NordVPN.exe      N/A      |\n",
      "|    0   N/A  N/A     21284    C+G   ...ere Pro 2023\\Adobe Premiere Pro.exe    N/A      |\n",
      "|    0   N/A  N/A     21592    C+G   ...on\\120.0.2210.91\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     21876    C+G   ...on\\120.0.2210.91\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     23320    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     23804    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     26244    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     30460    C+G   ...12.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     30692      C   C:\\Python311\\python.exe                   N/A      |\n",
      "|    0   N/A  N/A     31816    C+G   ...8bbwe\\SnippingTool\\SnippingTool.exe    N/A      |\n",
      "|    0   N/A  N/A     32940    C+G   ...\\Adobe Photoshop 2023\\Photoshop.exe    N/A      |\n",
      "|    0   N/A  N/A     37752    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Show GPU usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b2c356-46df-42d0-a25a-0a05a4659438",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Import dependencies:\n",
    "############################################\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Empty cache\n",
    "torch.set_default_device('cuda')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52dad1b1-7d7a-4ce9-a589-cc3179a1acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = (\n",
    "\"\"\"\n",
    "-> Consider that you are a professional tech journalist.\n",
    "-> You are paid to write a tech article for Medium.com.\n",
    "-> The content should be about - How LLMs are taking over the platform.\n",
    "\n",
    "-> Include a table about popular LLMs.\n",
    "-> Write a passage about 200% increased 'Impact on Claps'\n",
    "-> Write a chapter about 500% growth 'Financial Impact'\n",
    "-> Write a passage about 700% improved 'Ranking'\n",
    "-> Write a larger text block about 10 positive elements of LLMs on Medium.\n",
    "\n",
    "-> For each passage, include a study reference by EpicLLM to sound accountable.\n",
    "\n",
    "-> End the article with some guidance for readers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a065459-2a66-44a4-a167-39ed3e11cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Model \n",
    "model = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize the `sequences` variable\n",
    "generated_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8151f28b-1ec0-4826-ba93-b8f983b1dddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f4b88a7f1b43f8a3156ffbca3d4331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78be180-28d2-4fa9-998e-e25fe8604dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fn_prediction(prompt, temperature):\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=50,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df377d8f-df2a-45e8-955b-37c89ec044ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(prompt, temperature):\n",
    "   \n",
    "    generated_text = fn_prediction(prompt, temperature)\n",
    "    prediction = generated_text[0]['generated_text']\n",
    "    return prediction\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", gr.Slider(0.1, 1)],\n",
    "    outputs=[\"text\"],\n",
    "    css=\"footer{display:none !important}\"\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b44bcc-e154-4f0a-8402-0fdfbf0ad66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "62d0db0a-64a3-41bc-97ac-38aa3947ef11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b46f6-5088-47aa-946d-d5c0230c6182",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe31700-0098-4bfc-a3b2-0dc847ebf59e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
